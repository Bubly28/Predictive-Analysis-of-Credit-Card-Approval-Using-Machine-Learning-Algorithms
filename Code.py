# -*- coding: utf-8 -*-
"""Final Credit Approval Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dYLMyesyDclJ2-wZ8Y-kNOF8T_m7Expe
"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectFromModel

# fetch dataset
credit_approval = fetch_ucirepo(id=27)

# data (as pandas dataframes)
X = credit_approval.data.features
y = credit_approval.data.targets

# metadata
print(credit_approval.metadata)

# variable information
print(credit_approval.variables)

#Displaying the feature and target variable values
print(X)
print(y)

# Reverse the order of columns in DataFrame X
X_reversed = X.iloc[:, ::-1]
print(X_reversed)

# Concatenate reversed X (features) and y (target variable) into a single DataFrame
data = pd.concat([X_reversed, y], axis=1)

#Shape of the dataframe
data.shape

#Printing the head of the dataframe
data.head(10)

"""The dataset, obtained from the UCI machine learning repository with strict data confidentiality measures, has been anonymized to protect sensitive information. However, referencing the information available in the provided [blog](https://rstudio-pubs-static.s3.amazonaws.com/73039_9946de135c0a49daa7a0a9eda4a67a72.html), we can identify the specific variable names associated with the dataset. These variables include Gender, Age, Debt, Marital Status, Banking Customer Status, Education Level, Ethnicity, Years of Employment, Prior Default History, Employment Status, Credit Score, Driver's License Status, Citizenship, Zip Code, Income, and Approval Status."""

#Changing the varible names into meaningful values
data_cols = ['Gender', 'Age', 'Debt', 'MaritalStatus', 'BankCustomerStatus', 'EducationLevel', 'Ethnicity', 'YearsEmployed', 'PriorDefaultHistory', 'EmploymentStatus', 'CreditScore', 'DriversLicenseStatus','Citizen', 'ZipCode', 'Income', 'ApprovalStatus']
data.columns = data_cols

#Displaying the data after changing variable names
#Head
data.head(10)

#Tail
data.tail(10)

data.info()

data.describe()

"""# **Exploratory Data Analysis**

Univariate Analysis
"""

# Count the occurrences of each category in the target column
approval_status_counts = data['ApprovalStatus'].value_counts()

# Calculate the percentage of each category
total_samples = len(data)
approval_status_percentages = approval_status_counts / total_samples * 100

# Define colors for the bars
colors = '#4b0082'

# Plot the bar chart
plt.bar(approval_status_counts.index, approval_status_counts, color = colors)

# Add labels and title
plt.xlabel('Approval Status', size = 14)
plt.ylabel('Count', size = 14)
plt.title('Distribution of Approval Status', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Approval Status.png')
# Show the plot
plt.show()


# Print the count and percentage of each category
for status, count, percentage in zip(approval_status_counts.index, approval_status_counts, approval_status_percentages):
    print(f"{status}: Count = {count}, Percentage = {percentage:.2f}%")

# Count the occurrences of each category in the 'Gender' column
gender_counts = data['Gender'].value_counts()

# Calculate the percentage of each category
gender_percentages = gender_counts / total_samples * 100

# Define colors for the bars
colors = '#4b0082' # Purple shade

# Plot the bar chart with specified colors
plt.bar(gender_counts.index, gender_counts, color=colors)

# Add labels and title
plt.xlabel('Gender', size = 14)
plt.ylabel('Count', size = 14)
plt.title('Distribution of Gender', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Gender.png')
# Show the plot
plt.show()

# Print the count and percentage of each category
for gender, count, percentage in zip(gender_counts.index, gender_counts, gender_percentages):
    print(f"{gender}: Count = {count}, Percentage = {percentage:.2f}%")

# Define age categories
age_bins = np.arange(0, 81, 10)
age_labels = [f"{i}-{i+9}" for i in range(0, 80, 10)]  # Adjusted to have one fewer label

# Plot histogram for Age
plt.hist(data['Age'], bins=age_bins, color='#4b0082')

# Add labels and title
plt.xlabel('Age', size = 14)
plt.ylabel('Frequency', size = 14)
plt.title('Distribution of Applicant Ages', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Age.png')
# Show the plot
plt.show()

# Calculate the count and percentage of each age category
age_categories = pd.cut(data['Age'], bins=age_bins, labels=age_labels, include_lowest=True)
age_counts = age_categories.value_counts()
age_percentages = age_counts / total_samples * 100

# Print the count and percentage of each age category
for age_category, count, percentage in zip(age_counts.index, age_counts, age_percentages):
    print(f"Age Category {age_category}: Count = {count}, Percentage = {percentage:.2f}%")

# Count the occurrences of each category in the BankCustomer column
bank_customer_counts = data['BankCustomerStatus'].value_counts()

# Calculate the percentage of each category
total_samples = len(data)
bank_customer_percentages = bank_customer_counts / total_samples * 100

# Plot the bar chart with purple color
bank_customer_counts.plot(kind='bar', color='#4b0082')

# Add labels and title
plt.xlabel('Bank Customer Category', size = 14)
plt.ylabel('Count', size = 14)
plt.title('Distribution of Bank Customers Applying for Credit Card', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Bank Customer.png')
# Show the plot
plt.show()

# Print the count and percentage of each category
for category, count, percentage in zip(bank_customer_counts.index, bank_customer_counts, bank_customer_percentages):
    print(f"{category}: Count = {count}, Percentage = {percentage:.2f}%")

# Define the bins for years of employment
employment_bins = [0, 5, 10, 15, 20, 25, 30]
employment_labels = ['0-5 yrs', '5-10 yrs', '10-15 yrs', '15-20 yrs', '20-25 yrs', '25-30 yrs']

# Plot the histogram
plt.hist(data['YearsEmployed'], bins=employment_bins, color='#4b0082')

# Add labels and title
plt.xlabel('Years of Employment', size = 14)
plt.ylabel('Frequency', size = 14)
plt.title('Distribution of Years of Employment', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Years Employed.png')
# Show the plot
plt.show()

# Calculate the count and percentage of each category
employment_categories = pd.cut(data['YearsEmployed'], bins=employment_bins, labels=employment_labels, include_lowest=True)
employment_counts = employment_categories.value_counts()
employment_percentages = employment_counts / total_samples * 100

# Print the count and percentage of each category
for category, count, percentage in zip(employment_counts.index, employment_counts, employment_percentages):
    print(f"{category}: Count = {count}, Percentage = {percentage:.2f}%")

# Plotting histogram for income distribution
plt.hist(data['Income'], color='#4b0082', bins=10)

# Adding labels and title
plt.xlabel('Income', size = 14)
plt.ylabel('Frequency', size = 14)
plt.title('Income Distribution of Applicants', size = 18)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Income.png')
# Show the plot
plt.show()

# Define income categories
income_bins = [0, 10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]
income_labels = ['0-10000', '10001-20000', '20001-30000', '30001-40000', '40001-50000', '50001-60000', '60001-70000', '70001-80000', '80001-90000', '90001-100000']

# Count the occurrences of each income category
income_categories = pd.cut(data['Income'], bins=income_bins, labels=income_labels, include_lowest=True)
income_counts = income_categories.value_counts()
income_percentages = income_counts / total_samples * 100

# Print the count and percentage of each income category
for category, count, percentage in zip(income_counts.index, income_counts, income_percentages):
    print(f"{category}: Count = {count}, Percentage = {percentage:.2f}%")

"""Bivariate Analysis"""

# Create a crosstab for Gender vs Approval Status
gender_approval_ct = pd.crosstab(data['Gender'], data['ApprovalStatus'])

# Define new colors for the bars
dark_purple_color = '#4B0082'  # Dark purple shade
pastel_pink_color = '#FFC0CB'  # Pastel pink shade

# Plot the crosstab with colors and set figure size
ax = gender_approval_ct.plot(kind='bar',
                             color=[dark_purple_color, pastel_pink_color],
                             figsize=(10, 7))

# Add labels and title
ax.set_xlabel('Gender', size = 14)
ax.set_ylabel('Count', size = 14)
ax.set_title('Relationship Between Gender and Approval Status', size =18)

# Rotate the x-axis labels to show them horizontally
plt.xticks(size = 14, rotation=0)
plt.yticks(size = 14)

# Calculate the percentages and annotate the bars
for i, bar in enumerate(ax.patches):
    # Get the total samples for the gender category
    gender = 'a' if i < len(gender_approval_ct) else 'b'
    total_for_gender = gender_approval_ct.loc[gender].sum()

    # Calculate the percentage
    percentage = (bar.get_height() / total_for_gender) * 100

    # Annotate the bar with the percentage
    ax.annotate(f'{percentage:.2f}%', (bar.get_x() + bar.get_width() / 2, bar.get_height()), ha='center', va='bottom', size = 14)

# Saving the plot image
plt.savefig('Gender vs Approval Status.png')
# Show the plot
plt.show()

colors = ['#9932CC', '#FFC0CB']  # Dark purple and pink

# Create a crosstabulation between Approval Status and Education Level
approval_education_ct = pd.crosstab(data['ApprovalStatus'], data['EducationLevel'])

# Plotting the crosstabulation with Approval Status on the x-axis and custom colors
fig, ax = plt.subplots(figsize=(10, 7))
approval_education_ct.T.plot(kind='bar', stacked=True, color=colors, ax=ax)

# Customize the plot with labels, title, and rotated x-axis labels for clarity
ax.set_xlabel('Education Level', size = 14)
ax.set_ylabel('Count', size = 14)
ax.set_title('Relationship Between Approval Status and Education Level', size = 18)
plt.xticks(size = 14, rotation=0)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Education Level vs Approval Status.png')
# Display the plot
plt.show()

colors = ['#4B0082', '#FFC0CB']  # Dark purple and pastel pink

# Recreate the crosstab
prior_default_approval_ct = pd.crosstab(data['ApprovalStatus'], data['PriorDefaultHistory'])

# Plotting the bar chart using the new colors
ax = prior_default_approval_ct.plot(kind='bar',
                                    color=colors,
                                    figsize=(10, 7))

# Set the labels and title
ax.set_xlabel('Approval Status', size = 14)
ax.set_ylabel('Count', size = 14)
ax.set_title('Relationship Between Prior Default and Approval Status', size = 18)

# Rotate the x-axis labels to show them horizontally
plt.xticks(size = 14, rotation=0)
plt.yticks(size = 14)

# Annotate the bars with the count
for p in ax.patches:
    ax.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005), size = 14)

# Saving the plot image
plt.savefig('Prior Default  vs Approval Status.png')
# Show the plot
plt.show()

# Creating intervals for the CreditScore using pd.cut
bins = range(0, max(data['CreditScore']) + 10, 10)  # Adjust the range and bin width as necessary
labels = [f'{i}-{i+10}' for i in bins[:-1]]
data['CreditScoreBin'] = pd.cut(data['CreditScore'], bins=bins, labels=labels, right=False)

# Creating a crosstab with the binned CreditScore
CreditScore = pd.crosstab(data['CreditScoreBin'], data['ApprovalStatus'])

# Normalize the cross-tabulation by row
CreditScore_normalized = CreditScore.div(CreditScore.sum(1).astype(float), axis=0)

# Plot with specified colors in shades of purple and pink
colors = ['#4B0082', '#FFC0CB']  # A gradient of purple and pink colors

# Plotting
CreditScore_normalized.plot(kind='bar', stacked=False, color=colors, figsize=(17, 10))

# Rotating x-axis labels for better readability
plt.xticks(size = 14, rotation=0)
plt.yticks(size = 14)
# Place the legend inside the plot area at the upper left, but inside the plot
plt.legend(title='ApprovalStatus', loc='upper right')

# Setting the labels and title
plt.xlabel('Credit Score Intervals', size = 14)
plt.ylabel('Proportion of Approval Status', size =14)
plt.title('Proportion of Approval Status for Credit Score Intervals', size = 18)
# Saving the plot image
plt.savefig('Credit Score  vs Approval Status.png')
# Show the plot
plt.show()

data.drop('CreditScoreBin', axis=1, inplace=True)
data.head()

colors = ['#D8BFD8', '#DDA0DD']

# Create separate data for each category of 'DriversLicense'
license_groups = data.groupby('DriversLicenseStatus')

# Creating a figure and axes for the pie charts
fig, axs = plt.subplots(1, 2, figsize=(14, 7))

for ax, (license_status, group) in zip(axs, license_groups):
    status_counts = group['ApprovalStatus'].value_counts()
    ax.pie(status_counts, labels=status_counts.index, autopct='%1.1f%%', startangle=90, colors=colors)
    ax.set_title(f'Has Driver\'s License: {license_status}', size = 14)

plt.suptitle('Credit Card Approval by Driver\'s License Status', size = 18)
# Saving the plot image
plt.savefig('Driver\'s License  vs Approval Status.png')
plt.show()

# Create a countplot for Credit Card Approval Status by Citizenship
plt.figure(figsize=(12, 6))
sns.countplot(x='Citizen', hue='ApprovalStatus', data=data, palette={'+': '#4B0082', '-': 'pink'})

plt.title('Credit Card Approval Status by Citizenship (Count)', size = 18)
plt.xlabel('Citizenship', size = 14)
plt.ylabel('Count', size = 14)
plt.legend(title='Approval Status')
plt.xticks(size = 14, rotation=0)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Approval Status vs Citizenship.png')
plt.show()

# Set the color palette
sns.set_palette('rocket')

# Selecting only numerical columns
numeric_data = data.select_dtypes(include='number')

# Creating pairplot
sns.pairplot(data=numeric_data)
plt.suptitle('Pairplot of Numerical Attributes', y=1.02, size = 25)
plt.xticks(size = 14)
plt.yticks(size = 14)
# Saving the plot image
plt.savefig('Pairplot.png')
plt.show()

"""# **Data Preprocessing**"""

# Check for missing values in the entire DataFrame
missing_values = data.isnull().sum()

# Plotting the missing values
plt.figure(figsize=(15, 6))
sns.barplot(x=missing_values.index, y=missing_values.values, color = '#DDA0DD')
plt.xticks(size = 14, rotation=90)
plt.yticks(size = 14)
plt.xlabel('Features', size = 14)
plt.ylabel('Number of Missing Values', size = 14)
plt.title('Missing Values in Each Column', size = 25)
plt.savefig('Missing Values.png')
plt.show()

print(missing_values)

# For numerical columns, fill missing values with the mean
for col in data.select_dtypes(include=['float64', 'int64']).columns:
    data[col] = data[col].fillna(data[col].mean())

# For categorical columns, fill missing values with the mode (the most frequently occurring value)
for col in data.select_dtypes(include=['object']).columns:
    data[col] = data[col].fillna(data[col].mode()[0])

# Check if there are any missing values left
print(data.isnull().sum())

# Initialize LabelEncoder
lbl_en = LabelEncoder()

for col in data.columns:
    if data[col].dtypes=='object':
        data[col]=lbl_en.fit_transform(data[col])

#Displaying the head
data.head()

# Define a function to handle outliers by capping them to lower and upper bounds
def handle_outliers(series):
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)

    # Calculate IQR (Interquartile Range)
    IQR = Q3 - Q1

    # Find lower and upper bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Cap outliers to lower and upper bounds
    series = series.clip(lower=lower_bound, upper=upper_bound)

    return series

# Select numerical columns for outlier detection and handling
numerical_columns = ['Age', 'Debt', 'YearsEmployed', 'CreditScore', 'ZipCode', 'Income']

# Count outliers before handling
outliers_before = (data[numerical_columns] < data[numerical_columns].quantile(0.25) - 1.5 * (data[numerical_columns].quantile(0.75) - data[numerical_columns].quantile(0.25))) | (data[numerical_columns] > data[numerical_columns].quantile(0.75) + 1.5 * (data[numerical_columns].quantile(0.75) - data[numerical_columns].quantile(0.25)))
num_outliers_before = outliers_before.sum().sum()

# Handle outliers by capping them to lower and upper bounds
data_outliers_handled = data.copy()  # Create a copy of the original DataFrame
for column in numerical_columns:
    data_outliers_handled[column] = handle_outliers(data[column])

# Count outliers after handling
outliers_after = (data_outliers_handled[numerical_columns] < data_outliers_handled[numerical_columns].quantile(0.25) - 1.5 * (data_outliers_handled[numerical_columns].quantile(0.75) - data_outliers_handled[numerical_columns].quantile(0.25))) | (data_outliers_handled[numerical_columns] > data_outliers_handled[numerical_columns].quantile(0.75) + 1.5 * (data_outliers_handled[numerical_columns].quantile(0.75) - data_outliers_handled[numerical_columns].quantile(0.25)))
num_outliers_after = outliers_after.sum().sum()

# Print the number of outliers before and after handling
print("Number of outliers before handling:", num_outliers_before)
print("Number of outliers after handling:", num_outliers_after)

# Visualize outliers using box plots
plt.figure(figsize=(20, 6))
plt.subplot(1, 2, 1)
data[numerical_columns].boxplot()
plt.title('Box Plot of Numerical Features (Before Handling Outliers)', size =16)
plt.subplot(1, 2, 2)
data_outliers_handled[numerical_columns].boxplot()
plt.title('Box Plot of Numerical Features (After Handling Outliers)', size = 16)
plt.savefig('Outliers.png')
plt.show()

# Split the data into features (X) and target variable (y)
X = data_outliers_handled.drop(columns=['ApprovalStatus'])  # Features
y = data_outliers_handled['ApprovalStatus']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Print the shapes of the training and testing sets
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)

# Initialize the MinMaxScaler
scaler = MinMaxScaler()

# Fit the scaler on the training data and transform both training and testing data
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Convert the scaled arrays back to DataFrames
X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)

# Print the scaled training and testing data
print("Scaled Training Data:")
print(X_train_scaled_df.head())
print("\nScaled Testing Data:")
print(X_test_scaled_df.head())

"""# **Logistic Regression**"""

# Initialize the logistic regression model
logistic_reg_model = LogisticRegression()

# Train the model on the training data
logistic_reg_model.fit(X_train_scaled, y_train)

# Make predictions on the testing data
y_pred_lr = logistic_reg_model.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_lr = accuracy_score(y_test, y_pred_lr)
print("Accuracy:", accuracy_lr)

# Calculate confusion matrix
conf_matrix_lr = confusion_matrix(y_test, y_pred_lr)

# Printing the confusion matrix
print(conf_matrix_lr)

# Define the hyperparameters grid
param_grid_lr = {
    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter
    'penalty': ['l1', 'l2'],               # Penalty term
    'solver': ['liblinear']                # Solver for optimization problem
}

# Initialize logistic regression model
logistic_reg_model = LogisticRegression()

# Initialize GridSearchCV
grid_search_lr = GridSearchCV(estimator=logistic_reg_model, param_grid=param_grid_lr, cv=5, scoring='accuracy')

# Perform Grid Search Cross Validation
grid_search_lr.fit(X_train_scaled, y_train)

# Best parameters found during grid search
best_params_lr = grid_search_lr.best_params_
print("Best Parameters:", best_params_lr)

# Use the best estimator found during grid search
best_model_lr = grid_search_lr.best_estimator_

# Make predictions on the testing data using the best model
y_pred_lr = best_model_lr.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_lr  = accuracy_score(y_test, y_pred_lr)
print("Accuracy:", accuracy_lr)

#get correlations of each features in dataset
corrmat_lr = data.corr()
top_corr_features_lr = corrmat_lr.index
plt.figure(figsize=(14,9))
#plot heat map
g=sns.heatmap(data[top_corr_features_lr].corr(),annot=True,cmap="Purples")

#absolute value of correlcation with ApprovalStatus
approvalStatus_corr_lr = np.abs(corrmat_lr['ApprovalStatus'])
#selecting columns with correlaction of 0.19 and above
mask = approvalStatus_corr_lr >= 0.17
approvalStatus_corr_lr = approvalStatus_corr_lr[mask]
approvalStatus_corr_lr.drop(index=['BankCustomerStatus','MaritalStatus','ApprovalStatus'], inplace=True)
approvalStatus_corr_lr

# Selecting the features based on correlation
selected_features_lr = approvalStatus_corr_lr.index

# Extracting features and target variable
X = data[selected_features_lr]
y = data['ApprovalStatus']

# Splitting the data into training and testing sets
X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X, y, test_size=0.3, random_state=40)

# Initialize the logistic regression model with a higher max_iter value
logistic_reg_model = LogisticRegression(max_iter=1000)

# Train the model on the training data
logistic_reg_model.fit(X_train_lr, y_train_lr)

# Make predictions on the testing data
y_pred_lr = logistic_reg_model.predict(X_test_lr)

# Evaluate the model's performance
accuracy_lr = accuracy_score(y_test_lr, y_pred_lr)
print("Accuracy:", accuracy_lr)

# Calculate confusion matrix for test set
conf_matrix_test_lr = confusion_matrix(y_test_lr, y_pred_lr)

# Plot confusion matrix for test set
plt.figure(figsize=(10, 8))  # Adjust figure size
sns.heatmap(conf_matrix_test_lr, annot=True, fmt="d", cmap="Purples", cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],  # Adjusted for clarity
            yticklabels=['Actual Negative', 'Actual Positive'],  # Adjusted for clarity
            annot_kws={"size": 25})  # Increase size of the values inside the plot
plt.title('Confusion Matrix of Logistic Regression', size=20)
plt.xlabel('Predicted Labels', size=18)
plt.ylabel('True Labels', size=18)
plt.xticks(size=14)
plt.yticks(size=14)
plt.savefig('Conf_mat.png')
plt.show()
print(conf_matrix_test_lr)

# Evaluate model performance on training set
train_accuracy_lr = logistic_reg_model.score(X_train_lr, y_train_lr)
print("Training Accuracy:", train_accuracy_lr)

# Evaluate model performance on testing set
test_accuracy_lr = logistic_reg_model.score(X_test_lr, y_test_lr)
print("Testing Accuracy:", test_accuracy_lr)

# Calculate precision, recall, and F1-score for training data
train_precision_lr = precision_score(y_train_lr, logistic_reg_model.predict(X_train_lr))
train_recall_lr = recall_score(y_train_lr, logistic_reg_model.predict(X_train_lr))
train_f1_score_lr = f1_score(y_train_lr, logistic_reg_model.predict(X_train_lr))
train_roc_auc_lr = roc_auc_score(y_train_lr, logistic_reg_model.predict(X_train_lr))

# Calculate precision, recall, and F1-score for testing data
test_precision_lr = precision_score(y_test_lr, y_pred_lr)
test_recall_lr = recall_score(y_test_lr, y_pred_lr)
test_f1_score_lr = f1_score(y_test_lr, y_pred_lr)
test_roc_auc_lr = roc_auc_score(y_test_lr, y_pred_lr)

# Print the evaluation metrics
print("Evaluation Metrics:")
print("Training Data:")
print("  Accuracy:", train_accuracy_lr)
print("  Precision:", train_precision_lr)
print("  Recall:", train_recall_lr)
print("  F1-score:", train_f1_score_lr)
print("ROC AUC Score:", train_roc_auc_lr)

print("\nTesting Data:")
print("  Accuracy:", test_accuracy_lr)
print("  Precision:", test_precision_lr)
print("  Recall:", test_recall_lr)
print("  F1-score:", test_f1_score_lr)
print("ROC AUC Score:", test_roc_auc_lr)

"""# **Decision Tree**"""

# Initialize the decision tree model
decision_tree_model = DecisionTreeClassifier()

# Train the model on the training data
decision_tree_model.fit(X_train_scaled, y_train)

# Make predictions on the testing data
y_pred_dt = decision_tree_model.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print("Accuracy:", accuracy_dt)

#Calculate the confusion matrix
conf_matrix_dt = confusion_matrix(y_test, y_pred_dt)
#Printing the confusion matrix
print(conf_matrix_dt)

# Define the hyperparameters grid
param_grid_dt = {
    'max_depth': [3, 5, 7, 10],                 # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],            # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]               # Minimum number of samples required to be at a leaf node
}

# Initialize the decision tree classifier
dt_classifier = DecisionTreeClassifier(random_state=42)

# Initialize GridSearchCV with the classifier and hyperparameters grid
grid_search_dt = GridSearchCV(estimator=dt_classifier, param_grid=param_grid_dt, cv=5, scoring='accuracy')

# Perform Grid Search Cross Validation
grid_search_dt.fit(X_train_scaled, y_train)

# Best parameters found during grid search
best_params_dt = grid_search_dt.best_params_
print("Best Parameters:", best_params_dt)

# Use the best estimator found during grid search
best_model_dt = grid_search_dt.best_estimator_

# Make predictions on the testing data using the best model
y_pred_dt = best_model_dt.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print("Accuracy:", accuracy_dt)

# Initialize a decision tree classifier
decision_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1, min_samples_split=2, random_state=42)

# Train the decision tree classifier
decision_tree.fit(X_train_scaled, y_train)

# Get feature importances
feature_importances = decision_tree.feature_importances_

# Sort feature importances in descending order
indices = feature_importances.argsort()[::-1]

# Select the top k features (e.g., top 5)
k = min(5, len(X.columns))  # Ensure k is not greater than the number of columns in X
selected_features_indices = indices[:k]

# Print selected feature indices
print("Selected Feature Indices:", selected_features_indices)

# Get names of features based on their importance scores
selected_features = data.columns[selected_features_indices]

# Print selected features
print("Selected Features:", selected_features)

# Select only the top k features from the training and testing data
X_train_selected_dt = X_train.iloc[:, selected_features_indices]
X_test_selected_dt = X_test.iloc[:, selected_features_indices]

# Initialize a decision tree classifier
decision_tree = DecisionTreeClassifier(max_depth=5, min_samples_leaf=1, min_samples_split=2, random_state=42)

# Train the decision tree classifier on the selected features
decision_tree.fit(X_train_selected_dt, y_train)

# Make predictions on the testing data using the model trained on selected features
y_pred_selected_dt = decision_tree.predict(X_test_selected_dt)

# Evaluate the model's performance
accuracy_selected_dt = accuracy_score(y_test, y_pred_selected_dt)
print("Accuracy with Selected Features:", accuracy_selected_dt)

# Calculate confusion matrix for test set with selected features
conf_matrix_selected_dt = confusion_matrix(y_test, y_pred_selected_dt)

# Plot confusion matrix for test set with selected features
plt.figure(figsize=(10, 8))  # Adjust figure size
sns.heatmap(conf_matrix_selected_dt, annot=True, fmt="d", cmap="Purples", cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],  # Adjusted for clarity
            yticklabels=['Actual Negative', 'Actual Positive'],  # Adjusted for clarity
            annot_kws={"size": 25})  # Increase size of the values inside the plot
plt.title('Confusion Matrix of Decision Tree', size=20)
plt.xlabel('Predicted Labels', size=18)
plt.ylabel('True Labels', size=18)
plt.xticks(size=14)
plt.yticks(size=14)
plt.savefig('Conf_mat_DT.png')
plt.show()
print(conf_matrix_selected_dt)

# Make predictions on the training data
y_train_pred_dt = decision_tree.predict(X_train_selected_dt)

# Calculate the accuracy on the training data
train_accuracy_dt = accuracy_score(y_train, y_train_pred_dt)
print("Training Accuracy:", train_accuracy_dt)

# Calculate the accuracy on the testing data
test_accuracy_dt = accuracy_score(y_test, y_pred_selected_dt)
print("Testing Accuracy:", test_accuracy_dt)

# Initialize the decision tree classifier
decision_tree = DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_samples_split=2, random_state=42)

# Perform k-fold cross-validation
num_folds = 5  # Number of folds
cv_scores = cross_val_score(decision_tree, X_train, y_train, cv=num_folds)

# Print the cross-validation scores
print("Cross-Validation Scores:", cv_scores)

# Calculate and print the mean and standard deviation of the cross-validation scores
mean_cv_score = cv_scores.mean()
std_cv_score = cv_scores.std()
print("Mean Cross-Validation Score:", mean_cv_score)
print("Standard Deviation of Cross-Validation Scores:", std_cv_score)

# Calculate evaluation metrics for training set
train_accuracy_dt = accuracy_score(y_train,  y_train_pred_dt)
train_precision_dt = precision_score(y_train,  y_train_pred_dt)
train_recall_dt = recall_score(y_train,  y_train_pred_dt)
train_f1_score_dt = f1_score(y_train,  y_train_pred_dt)
train_roc_auc_dt = roc_auc_score(y_train,  y_train_pred_dt)

# Calculate evaluation metrics for test set
test_accuracy_dt = accuracy_score(y_test, y_pred_selected_dt)
test_precision_dt = precision_score(y_test, y_pred_selected_dt)
test_recall_dt = recall_score(y_test, y_pred_selected_dt)
test_f1_score_dt = f1_score(y_test, y_pred_selected_dt)
test_roc_auc_dt = roc_auc_score(y_test,  y_pred_selected_dt)

# Print evaluation metrics
print("Evaluation Metrics for Training Set:")
print("Accuracy:", train_accuracy_dt)
print("Precision:", train_precision_dt)
print("Recall:", train_recall_dt)
print("F1-score:", train_f1_score_dt)
print("ROC AUC Score:", train_roc_auc_dt)
print("\n")

print("Evaluation Metrics for Test Set:")
print("Accuracy:", test_accuracy_dt)
print("Precision:", test_precision_dt)
print("Recall:", test_recall_dt)
print("F1-score:", test_f1_score_dt)
print("ROC AUC Score:", test_roc_auc_dt)

"""# **Random Forest**"""

# Initialize the Random Forest classifier
random_forest_model = RandomForestClassifier(random_state=42)

# Train the model on the training data
random_forest_model.fit(X_train_scaled, y_train)

# Make predictions on the testing data
y_pred_rf = random_forest_model.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Accuracy:", accuracy_rf)

# Define the hyperparameters grid
param_grid_rf = {
    'n_estimators': [100, 200, 300],        # Number of trees in the forest
    'max_depth': [None, 10, 20, 30],        # Maximum depth of the trees
    'min_samples_split': [2, 5, 10],        # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4]           # Minimum number of samples required to be at a leaf node
}

# Initialize the Random Forest classifier
random_forest_model = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV with the classifier and hyperparameters grid
grid_search_rf = GridSearchCV(estimator=random_forest_model, param_grid=param_grid_rf, cv=5, scoring='accuracy')

# Perform Grid Search Cross Validation
grid_search_rf.fit(X_train_scaled, y_train)

# Best parameters found during grid search
best_params_rf = grid_search_rf.best_params_
print("Best Parameters:", best_params_rf)

# Use the best estimator found during grid search
best_model_rf = grid_search_rf.best_estimator_

# Make predictions on the testing data using the best model
y_pred_rf = best_model_rf.predict(X_test_scaled)

# Evaluate the model's performance
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print("Accuracy:", accuracy_rf)

# Initialize Random Forest classifier
random_forest_model = RandomForestClassifier(max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200)

# Train the model on the training data
random_forest_model.fit(X_train_scaled, y_train)

# Extract feature importances
feature_importances = random_forest_model.feature_importances_

# Create a selector object that will use the random forest classifier to identify
# features whose importance is greater than or equal to a specified threshold
feature_selector = SelectFromModel(random_forest_model, threshold='median')

# Fit the selector to the data
feature_selector.fit(X_train_scaled, y_train)

# Transform the data to include only the selected features
X_train_selected = feature_selector.transform(X_train_scaled)
X_test_selected = feature_selector.transform(X_test_scaled)

# Get the selected feature indices
selected_feature_indices = feature_selector.get_support(indices=True)

# Check if the selected feature indices are within bounds
selected_feature_indices = [i for i in selected_feature_indices if i < len(X.columns)]

# Get the names of the selected features
selected_features = X.columns[selected_feature_indices]

# Print the selected features
print("Selected Features:", selected_features)

# Initialize the RandomForestClassifier with the best hyperparameters
random_forest_model = RandomForestClassifier(max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200)

# Train the model on the training data with selected features
random_forest_model.fit(X_train_selected, y_train)

# Make predictions on the testing data with selected features
y_pred_rf_selected = random_forest_model.predict(X_test_selected)

# Evaluate the model's performance
accuracy_rf_selected = accuracy_score(y_test, y_pred_rf_selected)
print("Accuracy with Selected Features:", accuracy_rf_selected)

# Calculate confusion matrix for test set with selected features
conf_matrix_rf_selected = confusion_matrix(y_test, y_pred_rf_selected)

# Plot confusion matrix for test set with selected features
plt.figure(figsize=(10, 8)) # Adjust figure size
sns.heatmap(conf_matrix_rf_selected, annot=True, fmt="d", cmap="Purples", cbar=False,
            xticklabels=['Predicted Negative', 'Predicted Positive'],  # Adjusted for clarity
            yticklabels=['Actual Negative', 'Actual Positive'],  # Adjusted for clarity
            annot_kws={"size": 25})  # Increase size of the values inside the plot
plt.title("Confusion Matrix of Random Forest", size=20)
plt.xlabel('Predicted Labels', size=18)
plt.ylabel('True Labels', size=18)
plt.xticks(size=14)
plt.yticks(size=14)
plt.savefig('Conf_mat_RF.png')
plt.show()
print(conf_matrix_rf_selected)

# Train the model with the selected features from the training set
best_model_rf.fit(X_train_selected, y_train)

# predictions and evaluations can proceed with the correctly matched feature sets
y_train_pred = best_model_rf.predict(X_train_selected)

print("Training Set Evaluation:")
print("Accuracy:", accuracy_score(y_train, y_train_pred))
print("Precision:", precision_score(y_train, y_train_pred))
print("Recall:", recall_score(y_train, y_train_pred))
print("F1 Score:", f1_score(y_train, y_train_pred))
print("ROC AUC Score:", roc_auc_score(y_train, best_model_rf.predict_proba(X_train_selected)[:, 1]))

# Evaluate the model on the testing set
print("\nTesting Set Evaluation:")
print("Accuracy with Selected Features:", accuracy_rf_selected)
print("Precision:", precision_score(y_test, y_pred_rf_selected))
print("Recall:", recall_score(y_test, y_pred_rf_selected))
print("F1 Score:", f1_score(y_test, y_pred_rf_selected))
print("ROC AUC Score:", roc_auc_score(y_test, best_model_rf.predict_proba(X_test_selected)[:, 1]))

# Perform cross-validation
cv_scores = cross_val_score(best_model_rf, X_train_selected, y_train, cv=5, scoring='accuracy')

# Calculate the mean and standard deviation of the cross-validation scores
cv_mean = cv_scores.mean()
cv_std = cv_scores.std()

print(f"CV Accuracy Mean: {cv_mean:.4f}")
print(f"CV Accuracy Standard Deviation: {cv_std:.4f}")

# Already computed accuracy for Logistic Regression, Decision Tree, and Random Forest
accuracies = [accuracy_lr, accuracy_dt, accuracy_rf_selected]
precisions = [precision_score(y_test, y_pred_lr), precision_score(y_test, y_pred_dt), precision_score(y_test, y_pred_rf_selected)]
recalls = [recall_score(y_test, y_pred_lr), recall_score(y_test, y_pred_dt), recall_score(y_test, y_pred_rf_selected)]
f1_scores = [f1_score(y_test, y_pred_lr), f1_score(y_test, y_pred_dt), f1_score(y_test, y_pred_rf_selected)]

model_names = ['Logistic Regression', 'Decision Tree', 'Random Forest']

# Create a DataFrame to display the metrics
metrics_df = pd.DataFrame({
    'Model': model_names,
    'Accuracy': accuracies,
    'Precision': precisions,
    'Recall': recalls,
    'F1 Score': f1_scores
})

metrics_df.set_index('Model', inplace=True)
print(metrics_df)
colors = ['#4B0082', '#DDA0DD', '#D8BFD8' , '#FFC0CB']

# Plotting the metrics for comparison
metrics_df.plot(kind='bar', figsize=(15, 7), color = colors)
plt.title('Performance Comparison of ML Models', size = 25)
plt.xlabel('Models', size =14)
plt.ylabel('Score', size =14)
plt.xticks(size =14, rotation=90)
plt.yticks (size =14)
plt.legend(loc='lower right')
plt.savefig('Performance Comparison.png')
plt.show()